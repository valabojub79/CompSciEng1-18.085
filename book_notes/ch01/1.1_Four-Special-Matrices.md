# 1.1: Four Special Matrices
## Key terms
- **Toeplitz matrix**: a matrix with a constant value along each diagonal
- **Positive definite**: quality of matrix with all *positive eigenvalues*
- **Positive semidefinite**: quality of a matrix with all *nonnegative eigenvalues*

## Key concepts
**Invertibility** is a property of a matrix that means it has an undefined 
inverse. This is equivalent to a **having a determinant of zero** since the 
determinant is the denominator in the formula for the matrix inverse, and thus 
a determinant of zero prevents the inverse from being defined.
A **non-invertible** matrix is **singular**. A matrix is invertible if and 
only if it has **all nonzero *pivots* and *eigenvalues***.

Critically, *all **positive definite** matrices are invertible* because 
all pivots and eigenvalues of a positive definite matrix are positive, so 
they can't be zero, and therefore the matrix is invertible. On the other hand, 
*not all **positive semidefinite** matrices are invertible* since they are 
permitted to have pivot(s) and eigenvalue(s) equal to zero.

### Solution to Ax = 0
For a non-singular, invertible matrix (nonzero determinant), the only solution 
vector *x* to this system is the zero vector. A singular, non-invertible matrix 
will have a nonzero solution vector *x* for this system. Each element (row) 
of the solution vector *x* functions as the coefficient for the corresponding 
column of matrix **A** in the dot/inner product that ends up as the 
corresponding row in the output vector (here, 0). That is, each element of the 
RHS vector (again, here, 0) is the linear combination of the columns of **A**, 
with the coefficients of the linear combination defined by the elements of the 
solution vector *x*. So the existence of a nonzero solution to **A***x* = *0* 
means that there's a non-trivial (not all-zeros) linear combination of the 
columns of **A** that will produce the zero vector. This, in turn, means 
that **the columns of A are not independent**! 

## Computational techniques
When solving a linear system, *avoid the determinant* since that 
tends to be more costly in terms of the computational work required than it 
is to get the matrix in row-reduced form via elimination. Furthermore, a 
*sparse coefficient matrix doesn't imply a sparse inverse*, so there could be 
substantial memory consumption involved in the representation of the inverse.
